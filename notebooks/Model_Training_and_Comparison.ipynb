{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "import os"
      ],
      "metadata": {
        "id": "1rB81DCpSncI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "EPOCHS = 50\n",
        "BATCH_SIZE = 128\n",
        "LEARNING_RATE = 0.001\n",
        "VALIDATION_SPLIT = 0.2\n",
        "EARLY_STOPPING_PATIENCE = 10"
      ],
      "metadata": {
        "id": "fYTcwDzmSnZ5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "DATASET_PATH = \"/kaggle/input/emnist/emnist-byclass-train.csv\"\n",
        "MODEL_OUTPUT_DIR = \"/kaggle/working/\"\n",
        "\n",
        "MAPPING = {\n",
        "    'digits': (0, 9),\n",
        "    'uppercase': (10, 35),\n",
        "    'lowercase': (36, 61)\n",
        "}"
      ],
      "metadata": {
        "id": "hGkWxmrmSpiK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class EarlyStopping:\n",
        "    def __init__(self, patience=7, verbose=False, delta=0, path='checkpoint.pth'):\n",
        "        self.patience = patience\n",
        "        self.verbose = verbose\n",
        "        self.counter = 0\n",
        "        self.best_score = None\n",
        "        self.early_stop = False\n",
        "        self.val_loss_min = np.Inf\n",
        "        self.delta = delta\n",
        "        self.path = path\n",
        "\n",
        "    def __call__(self, val_loss, model):\n",
        "        score = -val_loss\n",
        "        if self.best_score is None:\n",
        "            self.best_score = score\n",
        "            self.save_checkpoint(val_loss, model)\n",
        "        elif score < self.best_score + self.delta:\n",
        "            self.counter += 1\n",
        "            if self.verbose:\n",
        "                print(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n",
        "            if self.counter >= self.patience:\n",
        "                self.early_stop = True\n",
        "        else:\n",
        "            self.best_score = score\n",
        "            self.save_checkpoint(val_loss, model)\n",
        "            self.counter = 0\n",
        "\n",
        "    def save_checkpoint(self, val_loss, model):\n",
        "        if self.verbose:\n",
        "            print(f'Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}). Saving model ...')\n",
        "        torch.save(model.state_dict(), self.path)\n",
        "        self.val_loss_min = val_loss\n"
      ],
      "metadata": {
        "id": "fc5RbfGvSqJM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def filter_and_prepare_data(df, class_name):\n",
        "    start_label, end_label = MAPPING[class_name]\n",
        "    class_df = df[df[0].between(start_label, end_label)]\n",
        "\n",
        "    labels = class_df.iloc[:, 0].values - start_label\n",
        "\n",
        "    images = class_df.iloc[:, 1:].values.astype('float32') / 255.0\n",
        "    images = images.reshape(-1, 1, 28, 28)\n",
        "\n",
        "    return torch.tensor(images, dtype=torch.float32), torch.tensor(labels, dtype=torch.long)\n"
      ],
      "metadata": {
        "id": "emmpQyeFSqGQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class EMNISTDataset(Dataset):\n",
        "    def __init__(self, images, labels):\n",
        "        self.images = images\n",
        "        self.labels = labels\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.images[idx], self.labels[idx]"
      ],
      "metadata": {
        "id": "wX_SmHOSSuBt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CNNModel_Small(nn.Module):\n",
        "    def __init__(self, num_classes):\n",
        "        super(CNNModel_Small, self).__init__()\n",
        "        self.layer1 = nn.Sequential(\n",
        "            nn.Conv2d(1, 32, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(32), nn.ReLU(), nn.MaxPool2d(2))\n",
        "        self.layer2 = nn.Sequential(\n",
        "            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(64), nn.ReLU(), nn.MaxPool2d(2))\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.fc1 = nn.Sequential(nn.Linear(64 * 7 * 7, 128), nn.ReLU())\n",
        "        self.dropout = nn.Dropout(0.5)\n",
        "        self.fc2 = nn.Linear(128, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.layer1(x)\n",
        "        out = self.layer2(out)\n",
        "        out = self.flatten(out)\n",
        "        out = self.fc1(out)\n",
        "        out = self.dropout(out)\n",
        "        out = self.fc2(out)\n",
        "        return out\n",
        "\n",
        "class CNNModel_Medium(nn.Module):\n",
        "    def __init__(self, num_classes):\n",
        "        super(CNNModel_Medium, self).__init__()\n",
        "        self.conv_block1 = nn.Sequential(\n",
        "            nn.Conv2d(1, 32, kernel_size=3, padding=1), nn.BatchNorm2d(32), nn.ReLU(), nn.MaxPool2d(2))\n",
        "        self.conv_block2 = nn.Sequential(\n",
        "            nn.Conv2d(32, 64, kernel_size=3, padding=1), nn.BatchNorm2d(64), nn.ReLU(), nn.MaxPool2d(2))\n",
        "        self.conv_block3 = nn.Sequential(\n",
        "            nn.Conv2d(64, 128, kernel_size=3, padding=1), nn.BatchNorm2d(128), nn.ReLU(), nn.MaxPool2d(2))\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.fc_block = nn.Sequential(\n",
        "            nn.Linear(128 * 3 * 3, 256), nn.ReLU(), nn.Dropout(0.5))\n",
        "        self.classifier = nn.Linear(256, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.conv_block1(x)\n",
        "        out = self.conv_block2(out)\n",
        "        out = self.conv_block3(out)\n",
        "        out = self.flatten(out)\n",
        "        out = self.fc_block(out)\n",
        "        out = self.classifier(out)\n",
        "        return out\n",
        "\n",
        "class CNNModel_Large(nn.Module):\n",
        "    def __init__(self, num_classes):\n",
        "        super(CNNModel_Large, self).__init__()\n",
        "        self.conv_block1 = nn.Sequential(\n",
        "            nn.Conv2d(1, 32, kernel_size=3, padding=1), nn.BatchNorm2d(32), nn.ReLU(), nn.MaxPool2d(2))\n",
        "        self.conv_block2 = nn.Sequential(\n",
        "            nn.Conv2d(32, 64, kernel_size=3, padding=1), nn.BatchNorm2d(64), nn.ReLU(), nn.MaxPool2d(2))\n",
        "        self.conv_block3 = nn.Sequential(\n",
        "            nn.Conv2d(64, 128, kernel_size=3, padding=1), nn.BatchNorm2d(128), nn.ReLU(), nn.MaxPool2d(2))\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.fc_block = nn.Sequential(\n",
        "            nn.Linear(128 * 3 * 3, 512), nn.ReLU(), nn.Dropout(0.5),\n",
        "            nn.Linear(512, 256), nn.ReLU(), nn.Dropout(0.5))\n",
        "        self.classifier = nn.Linear(256, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.conv_block1(x); out = self.conv_block2(out); out = self.conv_block3(out)\n",
        "        out = self.flatten(out); out = self.fc_block(out); out = self.classifier(out)\n",
        "        return out\n"
      ],
      "metadata": {
        "id": "69_dPuyPSwf0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_and_save_expert_model(expert_name, model_class, num_classes, train_loader, val_loader):\n",
        "    print(\"-\" * 60)\n",
        "    print(f\"Training FINAL model for: {expert_name.upper()}\")\n",
        "    print(f\"Using Architecture: {model_class.__name__}\")\n",
        "    print(\"-\" * 60)\n",
        "\n",
        "    model = model_class(num_classes).to(DEVICE)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
        "\n",
        "    model_save_path = os.path.join(MODEL_OUTPUT_DIR, f\"{expert_name}_model.pth\")\n",
        "    early_stopper = EarlyStopping(patience=EARLY_STOPPING_PATIENCE, verbose=True, path=model_save_path)\n",
        "\n",
        "    for epoch in range(EPOCHS):\n",
        "        model.train()\n",
        "        for images, labels in train_loader:\n",
        "            images, labels = images.to(DEVICE), labels.to(DEVICE)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        model.eval()\n",
        "        running_val_loss = 0.0\n",
        "        with torch.no_grad():\n",
        "            for images, labels in val_loader:\n",
        "                images, labels = images.to(DEVICE), labels.to(DEVICE)\n",
        "                outputs = model(images)\n",
        "                loss = criterion(outputs, labels)\n",
        "                running_val_loss += loss.item() * images.size(0)\n",
        "\n",
        "        epoch_val_loss = running_val_loss / len(val_loader.dataset)\n",
        "        print(f\"Epoch {epoch+1}/{EPOCHS} | Validation Loss: {epoch_val_loss:.4f}\")\n",
        "\n",
        "        early_stopper(epoch_val_loss, model)\n",
        "        if early_stopper.early_stop:\n",
        "            print(\"Early stopping triggered!\")\n",
        "            break\n",
        "\n",
        "    print(f\"Final model for '{expert_name}' has been saved to {model_save_path}\\n\")\n"
      ],
      "metadata": {
        "id": "96Ynf7AcS5zq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UslLGq0tSjuc"
      },
      "outputs": [],
      "source": [
        "if __name__ == '__main__':\n",
        "    print(f\"Using device: {DEVICE}\")\n",
        "    print(\"Loading and preparing master dataset...\")\n",
        "    df = pd.read_csv(DATASET_PATH, header=None)\n",
        "    print(\"Dataset loaded.\")\n",
        "\n",
        "    final_model_setup = [\n",
        "        ('digits', CNNModel_Small),\n",
        "        ('uppercase', CNNModel_Medium),\n",
        "        ('lowercase', CNNModel_Medium)\n",
        "    ]\n",
        "\n",
        "    for expert_name, model_class in final_model_setup:\n",
        "        images, labels = filter_and_prepare_data(df, expert_name)\n",
        "        full_dataset = EMNISTDataset(images, labels)\n",
        "\n",
        "        start_label, end_label = MAPPING[expert_name]\n",
        "        num_classes = end_label - start_label + 1\n",
        "\n",
        "        val_size = int(len(full_dataset) * VALIDATION_SPLIT)\n",
        "        train_size = len(full_dataset) - val_size\n",
        "        train_dataset, val_dataset = random_split(full_dataset, [train_size, val_size])\n",
        "\n",
        "        train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "        val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
        "\n",
        "        train_and_save_expert_model(expert_name, model_class, num_classes, train_loader, val_loader)\n"
      ]
    }
  ]
}